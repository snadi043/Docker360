<!-- PROJECT SUMMARY -->

This project main focus will be on how an application which is created and developed locally on the developers local machine (environment) is configured to deploy in cloud and host the web application to the users. The complete details, architecture flow and tech stack are described below.

<!-- Tech Stack  -->

    - Node JS framework
    - Docker
    - Dockerhub
    - AWS EC2
    - SSH Configuration 
    - Github
    - REST API
   
<!-- ARCHITECTURE FLOW  -->

The main application which is basically a NODE based normal application which has REST API's and endpoints to build the communication in between the backend code and the local node server which handle CRUD operations using HTTP methods like GET,POST,UPDATE and DELETE.

To make all the version changes and code commits this project is integrated with GITHUB and the repository for the application is 
https://github.com/snadi043/Docker360.git

Also, to make this project easy to ship and make it able to run in the same way in all environments, it is build and integrated with Containerization Tool which is DOCKER and the image and container details are defined below.
    Docker image : docker-prod-container-cloud-deployment
    Docker container: docker-prod-cloud-deploy

Once, all the above steps are done, the application is then deployed in AWS cloud platform using EC2 service to configure the app with all the security, access control and hosting.
    Detailed Steps include:
        - Configuring an AWS EC2 Instance with all the specifications dealing with (OS, RAM, STORAGE, NETWORKING etc.)
        - Configuring the proper access to make sure who can use the application, in a way to control the traffic and implement a layer of security to the application.
        -  Launching the instance.

Finally, after the application is integrated with docker and the final image is locked, the application is then deployed into the above created AWS EC2 instance to host in the cloud using SSH (Secure Shell) and then install DOCKER in the EC2 Instance and run the image in that instance. The commands and the setup for the instance and the shell connection are defined below.
    - Instance ID - i-0b5f4225781edcef5 (node-app-cloud-deploy)
    - Open an SSH client. (For Linux and MacOS it is already enabled in the sytem).
    - Place the private key file generated when creating the instance in root directory of the project.
    - Run this command, if necessary, to ensure your key is not publicly viewable.
            chmod 400 "permission file name"
    - Connect to your instance using its Public DNS:
            ec2-3-147-79-144.us-east-2.compute.amazonaws.com

Example:
ssh -i "node-app-cloud-deploy.pem" ec2-user@ec2-3-147-79-144.us-east-2.compute.amazonaws.com

Installing the Docker application in the remote machine which is on above created AWS EC2 instance.
The steps to follow to install the docker remotely is as follows.
    - sudo yum update -y
    - sudo yum -y install docker
    - sudo service docker start
    - sudo usermod -a -G docker ec2-user
    # Make sure to log out + back in after running these commands.
    - sudo systemctl enable docker
    - docker version
After running the docker version command it has to display all the docker information installed in the remote istance.

Deploying the source code from the local machine into the remote machine which is the AWS EC2 instance created in the above steps.
- This process can be done in two steps
  1. Migrating the source code directly from the local machine to remote machine and then building an image and run the conatainer in the remote machine using the docker application which is installed previously with SSH commands.
  2. Building the docker image and the container in the local machine and then migrating the source code through the already build docker image to the dockerhub by creating a repository in the dockerhub and then running that image in our remote machine. (AWS EC2 instance).
- When using the step-1, there are lot of configurations and steps the developer has to deal with, which is cumbersome.
- So, the best and most optimized solution is to follow step-2.

Steps and Procedure involved while deploying the source code from local machine into the remote remote machine based on procedure explained in line 56.
    - Signing to your dockerhub account.
    - Create a new repository with the matchable project name.
    - Build an image in your local machine and then assign a tag to that image which matches the remote dockerhub project name.
    - Create a connection from the local docker terminal to dockerhub in the browser with username/password or code confirmation mechanism
    - Build a new image of the up to date source code and then attach a tag with the dockerhub repository name. 
    - Then, Push the image to the dockerhub with the displayed docker push command in the dockerhub.
            In this project it is as follows
            docker push snadi043/node-app-cloud-deploy:dockerhub

<!-- BOTTLENECKS IN THE PROJECT -->

While working with this project and in the process of launching the source code from the local machine to remote EC2 instance with dockerhub repository an issue was arised which is like a conflict in between the platform / OS in which dockerhub image is build and where this image is being used in the remote EC2 service which is platform / OS of remote AWS EC2 instance. The detailed "WARNINIG" is attached below.
        WARNING: image with reference snadi043/node-app-cloud-deploy:dockerhub was found but its platform (linux/arm64) does not match the specified platform (linux/amd64) docker.io/snadi043/node-app-cloud-deploy:dockerhub

To avoid this warning, the project image is build again with specific platform flag and the steps, commands are as follows: 
    - First checking the local machine OS and Platform specifications to match it with docker image OS and platform
    - In this case, it is apple m1 chip which is arm based, so a docker image with arm specifications has to be build.
  
    
    - Docker is installed in the remote AWS instance from within local machine and in the details of the installation, it is clearly shown that the details of docker application installed in AWS EC2 instance are as follows:
      - Client:
      Version:           25.0.8
      API version:       1.44
      Go version:        go1.24.2
      Git commit:        0bab007
      Built:             Thu May  8 00:00:00 2025
      OS/Arch:           linux/amd64
      Context:           default
  
    -   To build multi platform images in docker using the docker desktop or docker application itself there are two main procedures.
    -   The links below helps to create multi-platform docker images.
                https://docs.docker.com/build/building/multi-platform/
  
                https://forums.docker.com/t/docker-getting-started-tutorial-docker-no-matching-manifest-for-linux-amd64-in-the-manifest-list-entries/147304

    - Once a source is shipped from the local machine to dockerhub to then into remote AWS EC2 instance, we can still update the changes in the source code by just building a new image to the same dockerhub and then pushing that updated image into same remote AWS EC2 instance that was previously created.

                                            AWS Elastic Container Service (ECS)

  So far, all the details mentioned above is in a way can be called as (DIY) Do It Yourself process of end-to-end setup of an applicaiton using available infrastructure to host the web application into a domanin in AWS EC2 service to then can be accessed by the users.
    - This process is good and interesting to know th flow of the application development, building and launching but the difficult part here is not all the developer are keen/interested to explore and invest to know everything that is needed to make the application up and running for various reasons.
    - So, in such cases there is an alternative to replace the whole of environment/infrastrucutre setup in AWS which is by using AWS service called ECS (Elastic Container Service).
    - ECS -> It is one of the AWS Services mainly fousing on automating the remote servers in accordance with the source code image from the docker and keeps the application secure, scalable, optimized and traffic efficient and all of these functionalities are managed my AWS for us. So, in a way this process can be refered to as "MANAGED SERVICES".
    - There are mainly 4 important concepts within Elastic Container Service which are as follows.
                - Containers 
                - Tasks
                - Clusters
                - Services  
    - Also, this service is not free to use and the prices are based on the number of instances you configure to host. Other infrastructre pricing (storage, networking, servers etc). 
  
  Also, to emphasize on docker compose, the purpose of docker compose suites well in the projects which are scoped to run locally in the local machine where there is only single server is in communication and it is static all the time, but whereas in the case of launching an application using AWS there is no gaurantee that the same server is assigned all the time for the application to host it. So, everytime you make changes and push the image and run the containers in the cluster in the ECS service the server, public IP address changes. But, to execute multi-container commmunication, the same approach of making all the containers in the network talk to each other can be achieved by placing all the clusters in the ECS under a single task. However, the problem with assiginig the same server all the time is something which cannot be solved. Also, in addition to that, in the docker compose there is no flexibility to deal / configure the infrastructure specifications like (CPU, Storage, Mermory, Networks, OS etc) but in the process of deploying the application remotely in AWS all these additional information is neccessary and important. These are the limitations for the docker compose when it is to be utilized in the process of working with AWS ECS service. 
  
  Also, the concept of using the container name as the hostname to pick it up by the docker automatically to build the communication between the conatiners in multi-container communications using docker cannot be replicated when using ECS. So, the domain/host name has to be replaced with environment variable so that a value in the local environment is kept in the .env file and a new variable key value can be configured when creating a cluster in the ECS service. 